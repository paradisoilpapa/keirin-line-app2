# ==============================
# è²·ã„ç›®ï¼ˆæƒ³å®šçš„ä¸­ç‡ â†’ å¿…è¦ã‚ªãƒƒã‚º=1/pï¼‰  â€»â—ã‹ã‚‰â€œå›ºå®šè¡¨ç¤ºâ€
# ==============================


one = result_marks.get("â—", None)
two = result_marks.get("ã€‡", None)
three = result_marks.get("â–²", None)

if one is None:
    st.warning("â—æœªæ±ºå®šã®ãŸã‚è²·ã„ç›®ã¯ã‚¹ã‚­ãƒƒãƒ—")
    trioC_df = wide_df = qn_df = ex_df = santan_df = None
else:
    # baseï¼šSBãªã—ã‚¹ã‚³ã‚¢ â†’ softmax
    strength_map = dict(velobi_wo)
    xs = np.array([strength_map.get(i,0.0) for i in range(1, n_cars+1)], dtype=float)
    if xs.std() < 1e-12:
        base = np.ones_like(xs)/len(xs)
    else:
        z = (xs - xs.mean())/(xs.std()+1e-12)
        base = np.exp(z); base = base/base.sum()

    # å°â†’ç¢ºç‡ã®ã‚†ã‚‹æ ¡æ­£
    mark_by_car = {car: None for car in range(1, n_cars+1)}
    for mk, car in result_marks.items():
        if car is not None and 1 <= car <= n_cars:
            mark_by_car[car] = mk

    expo = 0.7 if confidence == "å„ªä½" else (1.0 if confidence == "äº’è§’" else 1.3)

    def calibrate_probs(base_vec: np.ndarray, stat_key: str) -> np.ndarray:
        m = np.ones(n_cars, dtype=float)
        for idx, car in enumerate(range(1, n_cars+1)):
            mk = mark_by_car.get(car)
            if mk not in RANK_STATS:
                mk = RANK_FALLBACK_MARK
            tgt = float(RANK_STATS[mk][stat_key])
            ratio = tgt / max(float(base_vec[idx]), 1e-9)
            m[idx] = float(np.clip(ratio**(0.5*expo), 0.25, 2.5))
        probs = base_vec * m
        probs = probs / probs.sum()
        return probs

    # åˆ¸ç¨®ã”ã¨ã®â€œã‚¤ãƒ™ãƒ³ãƒˆç¢ºç‡â€ç”¨ãƒ™ã‚¯ãƒˆãƒ«
    probs_p3 = calibrate_probs(base, "pTop3")  # ãƒ¯ã‚¤ãƒ‰ãƒ»ä¸‰é€£è¤‡
    probs_p2 = calibrate_probs(base, "pTop2")  # äºŒè»Šè¤‡
    probs_p1 = calibrate_probs(base, "p1")     # äºŒè»Šå˜ãƒ»ä¸‰é€£å˜

    rng = np.random.default_rng(20250830)
    trials = st.slider("ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³è©¦è¡Œå›æ•°", 1000, 20000, 8000, 1000)

    def sample_order_from_probs(pvec: np.ndarray) -> list[int]:
        # Plackettâ€“Luceé¢¨ã®Gumbelãƒã‚¤ã‚ºé †ä½æ±ºå®š
        g = -np.log(-np.log(np.clip(rng.random(len(pvec)), 1e-12, 1-1e-12)))
        score = np.log(pvec+1e-12) + g
        return (np.argsort(-score)+1).tolist()

    all_others = [i for i in range(1, n_cars+1) if i != one]

    # === ã‚«ã‚¦ãƒ³ãƒˆå™¨ï¼ˆâ—ã‹ã‚‰â€œå…¨ç›¸æ‰‹â€ï¼å›ºå®šè¡¨ç¤ºç”¨ï¼‰ ===
    wide_counts = {k:0 for k in all_others}
    qn_counts   = {k:0 for k in all_others}
    ex_counts   = {k:0 for k in all_others}
    st3_counts  = {}  # ä¸‰é€£å˜ï¼ˆâ—â†’ç›¸æ‰‹â†’å…¨ï¼‰ã¯å¾“æ¥ã©ãŠã‚Šï¼ˆå¿…è¦ãªã‚‰å›ºå®šè¡¨ç¤ºåŒ–ã‚‚å¯èƒ½ï¼‰

    # ä¸‰é€£è¤‡ã¯ã€Œâ—ï¼‹ä»»æ„ã®2é ­ï¼ˆå…¨ç›¸æ‰‹ï¼‰ã€ã®å…¨çµ„ã¿åˆã‚ã›
    trio_all = []
    for a_i in range(len(all_others)):
        for b_i in range(a_i+1, len(all_others)):
            a, b = all_others[a_i], all_others[b_i]
            t = tuple(sorted([one, a, b]))
            trio_all.append(t)
    trio_counts = {t:0 for t in trio_all}

    # === ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ ===
    for _ in range(trials):
        # p3ï¼šTop3ã‚¤ãƒ™ãƒ³ãƒˆï¼ˆãƒ¯ã‚¤ãƒ‰ãƒ»ä¸‰é€£è¤‡ï¼‰
        order_p3 = sample_order_from_probs(probs_p3)
        top3_p3 = set(order_p3[:3])
        if one in top3_p3:
            # ãƒ¯ã‚¤ãƒ‰ï¼šâ—-k ãŒã¨ã‚‚ã«Top3ãªã‚‰ã‚«ã‚¦ãƒ³ãƒˆ
            for k in all_others:
                if k in top3_p3:
                    wide_counts[k] += 1
            # ä¸‰é€£è¤‡ï¼šâ—ã‚’å«ã‚€Top3ãŒç¢ºå®šã—ãŸã¨ãã€ãã®çµ„åˆã›ãŒä¸€è‡´ãªã‚‰åŠ ç®—
            others = list(top3_p3 - {one})
            if len(others) == 2:
                a, b = sorted(others)
                t = tuple(sorted([one, a, b]))
                if t in trio_counts:
                    trio_counts[t] += 1

        # p2ï¼šTop2ã‚¤ãƒ™ãƒ³ãƒˆï¼ˆäºŒè»Šè¤‡ï¼‰
        order_p2 = sample_order_from_probs(probs_p2)
        top2_p2 = set(order_p2[:2])
        if one in top2_p2:
            for k in all_others:
                if k in top2_p2:
                    qn_counts[k] += 1

        # p1ï¼š1ç€ã‚¤ãƒ™ãƒ³ãƒˆï¼ˆäºŒè»Šå˜ãƒ»ä¸‰é€£å˜ï¼‰
        order_p1 = sample_order_from_probs(probs_p1)
        if order_p1[0] == one:
            k2 = order_p1[1]
            if k2 in ex_counts:
                ex_counts[k2] += 1
            # ä¸‰é€£å˜ã¯å¾“æ¥é‹ç”¨ï¼šâ—â†’[ç›¸æ‰‹]â†’å…¨ï¼ˆå›ºå®šè¡¨ç¤ºã«ã—ãŸã„å ´åˆã¯å…¨ç›¸æ‰‹ã¸æ‹¡å¼µå¯ï¼‰
            k3 = order_p1[2]
            if (k2 in all_others) and (k3 not in (one, k2)):
                st3_counts[(k2, k3)] = st3_counts.get((k2, k3), 0) + 1

    # ====== Pãƒ•ãƒ­ã‚¢ï¼ˆæœ€ä½æƒ³å®špï¼‰ã¨EVå¸¯ï¼ˆâ€œãƒãƒ©ãƒ³ã‚¹å¸¯â€ã¸è¡¨è¨˜å¤‰æ›´ï¼‰ ======
    P_FLOOR = globals().get("P_FLOOR", {
        "wide": 0.060, "sanpuku": 0.040, "nifuku": 0.050, "nitan": 0.040, "santan": 0.030
    })
    # å±•é–‹ã§è¤‡ç³»ã ã‘å¾®èª¿æ•´ï¼ˆÂ±10%ï¼‰
    scale = 1.00
    if confidence == "å„ªä½":   scale = 0.90
    elif confidence == "æ··ç·š": scale = 1.10
    for k in ("wide","sanpuku","nifuku"):
        P_FLOOR[k] *= scale

    E_MIN = globals().get("E_MIN", 0.00)   # æœŸå¾…å€¤ä¸‹é™ï¼ˆ0%ï¼‰
    E_MAX = globals().get("E_MAX", 0.50)   # æœŸå¾…å€¤ä¸Šé™ï¼ˆ+50%ï¼‰

    def _p_from_cnt(cnt: int) -> float:
        return cnt / trials if cnt>0 else 0.0

    # === ãƒ¯ã‚¤ãƒ‰ï¼ˆâ—-å…¨ï¼‰â€” å¸¸ã«å…¨ç›¸æ‰‹ã‚’è¡¨ç¤ºï¼ˆå›ºå®šè¡¨ç¤ºï¼‰ ===
    rows = []
    for k in sorted(all_others):
        p = _p_from_cnt(wide_counts[k])
        need = (1.0/p) if p>0 else None
        if p >= P_FLOOR["wide"] and need is not None:
            eval_txt = "ãƒãƒ©ãƒ³ã‚¹å¸¯ï¼ˆä¸‹é™ï¼‰"
            rows.append({
                "è²·ã„ç›®": f"{one}-{k}",
                "p(æƒ³å®šçš„ä¸­ç‡)": round(p, 4),
                "å¿…è¦ã‚ªãƒƒã‚º(=1/p)": round(need, 2),
                "è©•ä¾¡": eval_txt
            })
        else:
            rows.append({
                "è²·ã„ç›®": f"{one}-{k}",
                "p(æƒ³å®šçš„ä¸­ç‡)": round(p, 4),
                "å¿…è¦ã‚ªãƒƒã‚º(=1/p)": "-" if need is None else round(need,2),
                "è©•ä¾¡": "å¯¾è±¡å¤–ï¼ˆPãƒ•ãƒ­ã‚¢æœªæº€ï¼‰"
            })
    wide_df = pd.DataFrame(rows)
    st.markdown("#### ãƒ¯ã‚¤ãƒ‰ï¼ˆâ—-å…¨ï¼‰â€»è»Šç•ªé †ï¼ˆå›ºå®šè¡¨ç¤ºï¼‰")
    if len(wide_df) > 0:
        wide_df = wide_df.sort_values(by="è²·ã„ç›®", key=lambda s: s.map(_sort_key_by_numbers)).reset_index(drop=True)
        st.dataframe(wide_df, use_container_width=True)

    # === äºŒè»Šè¤‡ï¼ˆâ—-å…¨ï¼‰â€” å¸¸ã«å…¨ç›¸æ‰‹ã‚’è¡¨ç¤ºï¼ˆå›ºå®šè¡¨ç¤ºï¼‰ ===
    rows = []
    for k in sorted(all_others):
        p = _p_from_cnt(qn_counts[k])
        if p <= 0:
            rows.append({"è²·ã„ç›®": f"{one}-{k}", "p(æƒ³å®šçš„ä¸­ç‡)": 0.0, "ãƒãƒ©ãƒ³ã‚¹å¸¯": "-", "è©•ä¾¡": "å¯¾è±¡å¤–ï¼ˆPãƒ•ãƒ­ã‚¢æœªæº€ï¼‰"})
            continue
        need = 1.0/p
        low, high = need*(1.0+E_MIN), need*(1.0+E_MAX)
        if p >= P_FLOOR["nifuku"]:
            eval_txt = "ãƒãƒ©ãƒ³ã‚¹å¸¯"
            rows.append({
                "è²·ã„ç›®": f"{one}-{k}",
                "p(æƒ³å®šçš„ä¸­ç‡)": round(p, 4),
                "ãƒãƒ©ãƒ³ã‚¹å¸¯": f"{low:.1f}ã€œ{high:.1f}å€",
                "è©•ä¾¡": eval_txt
            })
        else:
            rows.append({
                "è²·ã„ç›®": f"{one}-{k}",
                "p(æƒ³å®šçš„ä¸­ç‡)": round(p, 4),
                "ãƒãƒ©ãƒ³ã‚¹å¸¯": "-",
                "è©•ä¾¡": "å¯¾è±¡å¤–ï¼ˆPãƒ•ãƒ­ã‚¢æœªæº€ï¼‰"
            })
    qn_df = pd.DataFrame(rows)
    st.markdown("#### äºŒè»Šè¤‡ï¼ˆâ—-å…¨ï¼‰â€»è»Šç•ªé †ï¼ˆå›ºå®šè¡¨ç¤ºï¼‰")
    if len(qn_df) > 0:
        qn_df = qn_df.sort_values(by="è²·ã„ç›®", key=lambda s: s.map(_sort_key_by_numbers)).reset_index(drop=True)
        st.dataframe(qn_df, use_container_width=True)

    # === äºŒè»Šå˜ï¼ˆâ—â†’å…¨ï¼‰â€” å¸¸ã«å…¨ç›¸æ‰‹ã‚’è¡¨ç¤ºï¼ˆå›ºå®šè¡¨ç¤ºï¼‰ ===
    rows = []
    for k in sorted(all_others):
        p = _p_from_cnt(ex_counts[k])
        if p <= 0:
            rows.append({"è²·ã„ç›®": f"{one}->{k}", "p(æƒ³å®šçš„ä¸­ç‡)": 0.0, "ãƒãƒ©ãƒ³ã‚¹å¸¯": "-", "è©•ä¾¡": "å¯¾è±¡å¤–ï¼ˆPãƒ•ãƒ­ã‚¢æœªæº€ï¼‰"})
            continue
        need = 1.0/p
        low, high = need*(1.0+E_MIN), need*(1.0+E_MAX)
        if p >= P_FLOOR["nitan"]:
            eval_txt = "ãƒãƒ©ãƒ³ã‚¹å¸¯"
            rows.append({
                "è²·ã„ç›®": f"{one}->{k}",
                "p(æƒ³å®šçš„ä¸­ç‡)": round(p, 4),
                "ãƒãƒ©ãƒ³ã‚¹å¸¯": f"{low:.1f}ã€œ{high:.1f}å€",
                "è©•ä¾¡": eval_txt
            })
        else:
            rows.append({
                "è²·ã„ç›®": f"{one}->{k}",
                "p(æƒ³å®šçš„ä¸­ç‡)": round(p, 4),
                "ãƒãƒ©ãƒ³ã‚¹å¸¯": "-",
                "è©•ä¾¡": "å¯¾è±¡å¤–ï¼ˆPãƒ•ãƒ­ã‚¢æœªæº€ï¼‰"
            })
    ex_df = pd.DataFrame(rows)
    st.markdown("#### äºŒè»Šå˜ï¼ˆâ—â†’å…¨ï¼‰â€»è»Šç•ªé †ï¼ˆå›ºå®šè¡¨ç¤ºï¼‰")
    if len(ex_df) > 0:
        ex_df = ex_df.sort_values(by="è²·ã„ç›®", key=lambda s: s.map(_sort_key_by_numbers)).reset_index(drop=True)
        st.dataframe(ex_df, use_container_width=True)

    # === ä¸‰é€£è¤‡ï¼ˆâ—-[ç›¸æ‰‹]-å…¨ï¼‰â€” å¸¸ã«å…¨ç›¸æ‰‹2é ­ã®çµ„åˆã›ã‚’è¡¨ç¤ºï¼ˆå›ºå®šè¡¨ç¤ºï¼‰ ===
    rows = []
    for t in sorted(trio_all, key=lambda name: _sort_key_by_numbers("-".join(map(str,name)))):
        cnt = trio_counts.get(t, 0)
        p = _p_from_cnt(cnt)
        if p <= 0:
            rows.append({"è²·ã„ç›®": f"{t[0]}-{t[1]}-{t[2]}", "p(æƒ³å®šçš„ä¸­ç‡)": 0.0, "ãƒãƒ©ãƒ³ã‚¹å¸¯": "-", "è©•ä¾¡": "å¯¾è±¡å¤–ï¼ˆPãƒ•ãƒ­ã‚¢æœªæº€ï¼‰"})
            continue
        need = 1.0 / p
        low, high = need*(1.0+E_MIN), need*(1.0+E_MAX)
        if p >= P_FLOOR["sanpuku"]:
            eval_txt = "ãƒãƒ©ãƒ³ã‚¹å¸¯"
            rows.append({
                "è²·ã„ç›®": f"{t[0]}-{t[1]}-{t[2]}",
                "p(æƒ³å®šçš„ä¸­ç‡)": round(p, 4),
                "ãƒãƒ©ãƒ³ã‚¹å¸¯": f"{low:.1f}ã€œ{high:.1f}å€",
                "è©•ä¾¡": eval_txt
            })
        else:
            rows.append({
                "è²·ã„ç›®": f"{t[0]}-{t[1]}-{t[2]}",
                "p(æƒ³å®šçš„ä¸­ç‡)": round(p, 4),
                "ãƒãƒ©ãƒ³ã‚¹å¸¯": "-",
                "è©•ä¾¡": "å¯¾è±¡å¤–ï¼ˆPãƒ•ãƒ­ã‚¢æœªæº€ï¼‰"
            })
    trioC_df = pd.DataFrame(rows)
    st.markdown("#### ä¸‰é€£è¤‡ï¼ˆâ—-[ç›¸æ‰‹]-å…¨ï¼‰â€»è»Šç•ªé †ï¼ˆå›ºå®šè¡¨ç¤ºï¼‰")
    if len(trioC_df) > 0:
        # æ–‡å­—åˆ—ã‚­ãƒ¼ç”¨ã®ä¸¦ã¹æ›¿ãˆ
        def _key_nums_tri(s): return list(map(int, re.findall(r"\d+", s)))
        trioC_df = trioC_df.sort_values(by="è²·ã„ç›®", key=lambda s: s.map(_key_nums_tri)).reset_index(drop=True)
        st.dataframe(trioC_df, use_container_width=True)

# ==============================
# noteç”¨ï¼šãƒ˜ãƒƒãƒ€ãƒ¼ã€œâ€œãƒãƒ©ãƒ³ã‚¹å¸¯â€ã¾ã¨ã‚
# ==============================
st.markdown("### ğŸ“‹ noteç”¨ï¼ˆâ€œãƒãƒ©ãƒ³ã‚¹å¸¯â€è¡¨ç¤ºï¼‰")

def _format_line_zone_note(name: str, bet_type: str, p: float) -> str | None:
    floor = P_FLOOR.get(bet_type, 0.03 if bet_type=="santan" else 0.0)
    if p < floor: return None
    need = 1.0 / max(p, 1e-12)
    if bet_type == "wide":
        return f"{name}ï¼š{need:.1f}å€ä»¥ä¸Šã§ãƒãƒ©ãƒ³ã‚¹"
    low, high = need*(1.0+E_MIN), need*(1.0+E_MAX)
    return f"{name}ï¼š{low:.1f}ã€œ{high:.1f}å€ï¼ˆãƒãƒ©ãƒ³ã‚¹å¸¯ï¼‰"

def _zone_lines_from_df(df: pd.DataFrame | None, bet_type_key: str) -> list[str]:
    if df is None or len(df) == 0 or "è²·ã„ç›®" not in df.columns:
        return []
    rows = []
    for _, r in df.iterrows():
        name = str(r["è²·ã„ç›®"])
        if "ãƒãƒ©ãƒ³ã‚¹å¸¯" in r and r["ãƒãƒ©ãƒ³ã‚¹å¸¯"] and r.get("è©•ä¾¡","").startswith("ãƒãƒ©ãƒ³ã‚¹"):
            rows.append((name, f"{name}ï¼š{r['ãƒãƒ©ãƒ³ã‚¹å¸¯']}" if bet_type_key!="wide" else f"{name}ï¼š{r.get('å¿…è¦ã‚ªãƒƒã‚º(=1/p)','-')}å€ä»¥ä¸Šã§ãƒãƒ©ãƒ³ã‚¹"))
    rows_sorted = sorted(rows, key=lambda x: _sort_key_by_numbers(x[0]))
    return [ln for _, ln in rows_sorted]

def _section_text(title: str, lines: list[str]) -> str:
    if not lines: return f"{title}\nå¯¾è±¡å¤–ï¼ˆPãƒ•ãƒ­ã‚¢æœªæº€ï¼‰"
    return f"{title}\n" + "\n".join(lines)

line_text = "ã€€".join([x for x in line_inputs if str(x).strip()])
score_order_text = " ".join(str(no) for no,_ in velobi_wo)
marks_line = " ".join(f"{m}{result_marks[m]}" for m in ["â—","ã€‡","â–²","â–³","Ã—","Î±","Î²"] if m in result_marks)

txt_trioC = _section_text("ä¸‰é€£è¤‡ï¼ˆâ—-[ç›¸æ‰‹]-å…¨ï¼‰",
                          _zone_lines_from_df(trioC_df, "sanpuku") if one is not None else [])
txt_wide  = _section_text("ãƒ¯ã‚¤ãƒ‰ï¼ˆâ—-å…¨ï¼‰",
                          _zone_lines_from_df(wide_df, "wide") if one is not None else [])
txt_qn    = _section_text("äºŒè»Šè¤‡ï¼ˆâ—-å…¨ï¼‰",
                          _zone_lines_from_df(qn_df, "nifuku") if one is not None else [])
txt_ex    = _section_text("äºŒè»Šå˜ï¼ˆâ—â†’å…¨ï¼‰",
                          _zone_lines_from_df(ex_df, "nitan") if one is not None else [])

note_text = (
    f"ãƒ©ã‚¤ãƒ³ã€€{line_text}\n"
    f"ã‚¹ã‚³ã‚¢é †ï¼ˆSBãªã—ï¼‰ã€€{score_order_text}\n"
    f"{marks_line}\n"
    f"\n"
    f"{txt_trioC}\n\n"
    f"{txt_wide}\n\n"
    f"{txt_qn}\n\n"
    f"{txt_ex}\n"
    "\nâ€»ã“ã®ã‚ªãƒƒã‚ºä»¥ä¸‹ã¯æœŸå¾…å€¤ï¼ˆEVï¼‰ä¸‹æŒ¯ã‚Œã‚’æƒ³å®šã€‚é›¢ã‚Œã‚‹ã»ã©ã«ç¢ºç‡â€”ã‚ªãƒƒã‚ºã®ãƒãƒ©ãƒ³ã‚¹ãŒå´©ã‚Œã€ãƒªã‚¹ã‚¯ãŒé«˜ã¾ã‚Šã¾ã™ã€‚\n"
    "â€»â–²ã¯ã€ŒæœŸå¾…å€¤èª¿æ•´æ ã€ã€‚çš„ä¸­ç‡ãŒä½ã™ãã‚‹å ´åˆã‚„ã‚ªãƒƒã‚ºãƒãƒ©ãƒ³ã‚¹ãŒæ‚ªã„å ´åˆã¯è²·ã„ç›®ã«åæ˜ ã•ã‚Œãªã„ã“ã¨ãŒã‚ã‚Šã¾ã™ï¼ˆå°ã¯ã‚·ã‚°ãƒŠãƒ«ï¼‰ã€‚\n"
    "â€»è¿”é‡‘ã¯å—ã‘ä»˜ã‘ã¦ãŠã‚Šã¾ã›ã‚“ã€‚ã”äº†æ‰¿ã®ä¸ŠãŠæ¥½ã—ã¿ãã ã•ã„ã€‚"
)
st.text_area("ã“ã“ã‚’é¸æŠã—ã¦ã‚³ãƒ”ãƒ¼", note_text, height=360)


